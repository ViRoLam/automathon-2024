
Training model:
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
CNNVideoClassifier                       [32, 1]                   55,488
├─Conv3d: 1-1                            [32, 16, 3, 256, 256]     4,336
├─BatchNorm3d: 1-2                       [32, 16, 3, 256, 256]     32
├─ReLU: 1-3                              [32, 16, 3, 256, 256]     --
├─MaxPool3d: 1-4                         [32, 16, 3, 128, 128]     --
├─Conv3d: 1-5                            [32, 32, 3, 128, 128]     13,856
├─BatchNorm3d: 1-6                       [32, 32, 3, 128, 128]     64
├─ReLU: 1-7                              [32, 32, 3, 128, 128]     --
├─MaxPool3d: 1-8                         [32, 32, 3, 64, 64]       --
├─Linear: 1-9                            [32, 512]                 201,327,104
├─ReLU: 1-10                             [32, 512]                 --
├─Linear: 1-11                           [32, 1]                   513
==========================================================================================
Total params: 201,401,393
Trainable params: 201,401,393
Non-trainable params: 0
Total mult-adds (G): 55.52
==========================================================================================
Input size (MB): 251.66
Forward/backward pass size (MB): 2416.05
Params size (MB): 805.38
Estimated Total Size (MB): 3473.09
==========================================================================================
Training...
X_shape: torch.Size([32, 10, 3, 256, 256])
/raid/home/automathon_2024/account8/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:605: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return F.conv3d(
  0%|          | 0/310 [00:01<?, ?it/s]
Traceback (most recent call last):
  File "/raid/home/automathon_2024/account8/automathon-2024/run.py", line 322, in <module>
    loss.backward()
  File "/raid/home/automathon_2024/account8/.local/lib/python3.10/site-packages/torch/_tensor.py", line 525, in backward
    torch.autograd.backward(
  File "/raid/home/automathon_2024/account8/.local/lib/python3.10/site-packages/torch/autograd/__init__.py", line 267, in backward
    _engine_run_backward(
  File "/raid/home/automathon_2024/account8/.local/lib/python3.10/site-packages/torch/autograd/graph.py", line 744, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn